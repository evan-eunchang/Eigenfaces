# Eigenfaces
Experimental facial recognition app  

## Abstract
Using applied linear algebra and principal component analysis (PCA), we can approximate an image of someone's face as a matrix and define this matrix in terms of the principal components. Using PCA, we can approximate
an image with far less data than an image, while still being able to recreate the image. In this implementation, we gathered a dataset of human faces. Then we found the eigenvectors of the covariance matrix of the dataset. 
These eigenvectors can be used to recreate every face in the database, and mostly recreate all other human faces as well. These eigenvectors are images themselves, of a principal component of the faces in the dataset. We call
them Eigenfaces. We can represent each face as a linear combination of these eigenfaces. By comparing the strength of each Eigenface in real human faces, we can find which faces are similar and are likely of the same person.
I implemented an Android Application to allow users to submit faces of their choosing, and for a new face, find the closest match of the faces already submitted.

## Mathematical Explanation
First, I'll explain the mathematical basis for this app and method.  
Even low-resolution images, for example the 112x92 grayscale images I used for this project, have an immense amount of data. The images in this project have over 10k pixels, which makes any kind of analysis on them very difficult and computationally expensive, not to mention the bad experience for the user. To make analysis of such objects easier, we can use principal component analysis (PCA) to find the components of greatest variation in our dataset, then represent each data point as a linear combination of these components at different magnitudes.  
One way of finding these principal components is compact Singular Value Decompostion (SVD), a process by which a matrix is decomposed into a semi-unitary matrix (U), a diagonal matrix of square roots of non-zero eigenvalues (S), and another semi-unitary matrix (VT). The detailed mathematical specifics of this process are not important, but the end result is that the transpose of the second semi-unitary matrix (V) is a collection of eigenvectors, our principal components. Compact SVD provides a useful alternative to finding the covariance matrix, a matrix that would be even larger than the one required for compact SVD. For compact SVD, we flatten each 112x92 image into a column vector of values 0..255, which represent the grayscale brightness. Then, we build a dataset matrix where there is a column for each image in our dataset, 400 columns for my dataset. Before we analyze this dataset matrix using SVD, we must center the variation at 0. We do this by finding the average face (see below) and subtracting it from each face in our dataset. This way, the SVD only analyzes variance from the "center". Now, the matrix is ready to be analyzed using a compact SVD program so that we can get the eigenvectors that represent the Eigenfaces. These Eigenfaces are our principal components, and the represent certain facial characteristics that make up a total face, for example dark or light eyes, or a full head of hair vs a bald head.  
![average face](app/src/main/res/drawable/eigenface_logo.png)  
With the Eigenfaces calculated, we can simply multiply a face we want to analyze by the eigenvectors, as the V matrix is essentially a coordinate transformation matrix (face_flattened @ V). face_flattened is 1x10304 (there are 10304 total pixels in my image) and V is 10304x400, so the end result is a column vector of 400 values of the strength of each Eigenface to represent the original image. 400 values is far less than the 10304 needed to represent the image with just pixels. These values are the magnitude of each component in the face analyzed, but we can think of them as coordinates. If we analyze the "distance" between each face using these coordinates, we can find the closest face in the Eigenface space and thus which face is the closest match.

## Implementation Through an Android Application
With the method locked down, I had to figure out how to implement it in an Android app. Since the app only had one main task, design was pretty simple. Using fragments, I split the app into four screens:  
+ A welcome screen with a RecyclerView showing the faces already analyzed
+ A screen where the user selects the image and the section of the image they'd like to analyze
+ A screen where analysis takes place and results are shown
+ A screen where, if the face submitted is the first or the app was wrong, the user can input a name  

The images were passed through ViewModels in the form of Bitmaps. All other data was passed in the form of strings, including the coordinate vectors, which were turned into csv strings.
I created a custom class, Portrait, that would contain a unique autogenerated ID, the name assigned to the face, the coordinate array in csv string format, and the name of the file that the image was saved to for later retrieval. All of the images were saved in the "files" folder within the app's file in the data directory on the device itself. Aside from the ID, all of these values were strings. This is because I saved the Portraits themselves in an Android Room Database so that I could easily access all the faces, and insert them with one line of code as well.
### Select Face Fragment
The idea of this fragment was simple: allow the user to select an image from the gallery and pan and zoom within that image to frame the exact face they want to analyze.  
Selecting an image from the gallery is a simple task that has been made very simple by Kotlin and Android infrastructure. I designed a custom ImageView class, MyImageView, to allow for panning and zooming while restricting the user from panning the image off the ImageView at all. This was accomplished using the dimensions of the image and the sscale factor in conjunction with the x and y positions of the image.  
Finally, I implemented a function within the fragment to create a bitmap of what was visible on the view, scale it to the dimensions required by our analysis (112x92), and convert it to grayscale. Then, I saved it to a ViewModel that could be accessed in other fragments so the user could see it again and the program could analyze it.
### Display Results Fragment
This fragment was also simple on the surface, but the program underneath was the most complicated in the entire application.  
Instead of calculating all the necessary matrices in the application, I calculated V_faces matrix and the average face vector based on the AT&T Cambridge Laboratory Database of Faces, in a python program, as they only needed to be calculated once. Then, I saved these matrices as csv files that I then imported into the app. I implemented functions to load these files in the fragment. Using these vectors and matrices, I implemented a function to calculate the coordinates using V_faces and the average face. Using these coordinates, I implemented a function to retreive the list of Portraits from the Room database and calculate the distance between the face being analyzed and every face in the list, using the norm of the difference between the face to be analyzed and the faces in the database. I also implemented functions to convert a coordinate array to a csv string and vice-versa for when arrays were saved to the Room database and when csv strings were retrieved.  
To perform the simple linear algebra operations still necessary (matrix multiplication and subtraction), I used the Multik library, which implements multi-dimensional arrays. I still had to implement a function to find the norm of a vector and matrix multiplication, so the only useful aspect of this library was the subtraction.  
Once again, if the face submitted was the first face ever submitted, the app would move straight to the naming fragment. If not, it would find the closest match and display it on screen with the name, and prompt the user to input whether the match was correct or not. If not, the app transitioned to the naming fragment. If correct, the image would be entered into the database as a Portrait with the same name as the match, the saved file name, and the coordinates calculated for it in csv format.
Regardless of correctness or if the face was the first to be submitted, the image of the face was saved to the device. If the app moved to the naming fragment, the file name and the coordinates in csv format would be passed through a ViewModel.
### Naming Fragment
Here, the user would be prompted to enter a name. Once they did and pressed the "Save" button, the image would be entered into the database as a Portrait with the inputted name, and the file name and coordinates passed in from the results fragment.

## Development Journey
I was inspired to make Eigenfaces after encountering this method partially coded in Python in a lab for my Linear Algebra class in college. After I decided to make Eigenfaces my next project, I began plotting out how I would put the app together and what smaller parts I would need to make it.  
### MyImageView
The first requirement that came to mind was the custom ImageView class, which would need to have zooming and panning features, while making sure the image filled the View at all times. I based my class off of a very basic zooming and panning ImageView on StackOverflow. I created the restrictions to keep the image in the View myself, and made an override function for setImageBitmap so Bitmaps could be used throughout the entire app. It was also during the design of this class that I experimented with saving images. Designing this class also introduced me to Bitmaps and was a good refresher for Android Studio.
### Files
The next problem I saw was the reading of the massive files for V_faces and avg_faces into the app to be used. After lots of experimentation, I settled on a scanner approach to read one line at a time to save memory. After a persistent bug that turned out to originate in my Python program, not Kotlin, my file readers were ready.
### Passing Values and Room Database
With the previous two problems solved, I was ready to begin designing the app proper. Designing the individual fragments would be simple, but implementing features that would allow them to pass values to each other was not. For the bitmap itself, I settled on a simple ViewModel. After some experimenting, I was able to make the ViewModel work within each fragment. There were not many online resources about this.  
With the Portraits themselves, I had to be able to not only share them between fragments, but save them for later access in both the same and different lifecycles of the app. These features could be implemented easily through a Room Database, but while Room Databases are simple to use, creating it was not. Implementing Room Databases within fragments was not well documented on the internet, so I had to do a lot of experimenting. Eventually I learned that I needed to use the same data access object (DAO) in each fragment, so the DAO would have to be a value in a companion object in the MainActivity, and initialized there as well.  
Finally, I had to ensure that Coroutines and LiveData observers did not continue operating after the user had moved to new fragments. Thanks to a custom observeOnce function, and reading up on Kotlin Job objects on the internet, I was able to solve these issues.

### Final Lessons and General Reflection
One of the main problems I had with designing this app was the lack of information on the internet. I was very fortunate to have most of the problems I had solved by others on the internet and their solutions ready for me to use. However, most of these solutions were in Java, which has some crucial differences from Kotlin, and frequently made extensive use of deprecated functions and features. Thus, I had to reinvent many solutions and figure many out for myself despite them already being solved. I ran into similar problems using Fragments, which have some small but crucial differences from activities. Almost every tutorial on the internet used Activities instead of Fragments, which led to me having to work hard to find ways to implement everything in Fragments on my own.  
Another large problem, and perhaps the largest challenge to all Android Developers, was getting the dependencies and libraries necessary for the app to work. These libraries are frequently being updated, all independently of each other, and they often conflict with each other or the Kotlin and Java foundations of the app. Additionally, certain versions of libraries may not cooperate with each other, while certain versions are perfectly fine, so configuring every version to comply with the others was difficult. Through careful calibration I was able to get this house of cards to stand.
In general terms, this project taught me a lot about problem solving, as despite many tutorials on the internet for problems similar to mine, I was the only one actually working on my app, so I needed to adapt those tutorials or create new solutions myself. Planning the entire app and then having to implement it, with any faults in the design having to be solved by me, was very educational in this way as well. With regards to planning, maintaining my own work schedule was daunting, but thanks to self-discipline, I did not fall behind my schedule and finished quickly.
As the only developer for this app, I was the one who would make it succeed or fail, a fact that was sometimes stressful but was also rewarding in the end.  

Thank you for exploring my app, and thanks to everyone below who helped me.

### Credits
Original method written by Professor Jer-Chin Chuang and the MATH257 Staff at the University of Illinois Urbana Champaign  
MyImageView class based on custom class created by [Hank on StackOverflow](https://stackoverflow.com/questions/12169905/zoom-and-panning-imageview-android)  
Model was trained on data from the AT&T Cambridge Laboratory
